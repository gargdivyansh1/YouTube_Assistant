# YouTube Assistant - AI-Powered Video Knowledge Engine

**YouTube Assistant** brings advanced, AI-powered conversational understanding to every YouTube video through a seamless browser extension and a sophisticated, scalable backend. It's more than just a Q&A bot: it's an extensible, distributed knowledge infrastructure for the world's video content.

> **Transform any YouTube video into an interactive knowledge source with AI-powered question answering, voice recognition, and planetary-scale vector caching.**

---

## Table of Contents

1. [Overview](#overview)
2. [Key Innovation: Planet-Scale Vector Caching](#key-innovation-planet-scale-vector-caching)
3. [Features](#features)
4. [Technology Stack](#technology-stack)
5. [Architecture](#architecture)
6. [Deep Technical Details](#deep-technical-details)
   - [Backend Architecture](#backend-architecture)
   - [Chrome Extension Architecture](#chrome-extension-architecture)
   - [Data Flow Diagram](#data-flow-diagram)
7. [Component-by-Component Breakdown](#component-by-component-breakdown)
   - [Transcript Acquisition Pipeline](#transcript-acquisition-pipeline)
   - [Semantic Chunking & Embedding](#semantic-chunking--embedding)
   - [Vector Store Management](#vector-store-management)
   - [Query Intelligence & Retrieval](#query-intelligence--retrieval)
8. [Setup & Installation](#setup--installation)
9. [Configuration](#configuration)
10. [API Reference](#api-reference)
11. [Advanced Topics](#advanced-topics)
12. [Troubleshooting](#troubleshooting)
13. [Contributing](#contributing)
14. [Acknowledgements](#acknowledgements)

---

## Overview

YouTube Assistant enables users to ask **any question** about any YouTube video and receive **context-rich, intelligent answers** directly within the YouTube interface. Unlike simple transcript overlays or generic AI chatbots, this assistant:

- **Processes video content once** and makes it available to millions of users
- **Understands context** through semantic embeddings and retrieval-augmented generation
- **Supports multi-turn conversations** with full chat history awareness
- **Handles both broad and specific queries** with intelligent query classification
- **Works offline from captions** using automatic speech recognition (ASR) when needed
- **Scales globally** with a single vector cache per video serving unlimited users

### Real-World Use Cases

- ğŸ“š **Educational Content**: Students ask questions about lecture videos, tutorials, and educational content
- ğŸ¬ **Entertainment**: Viewers get deep explanations about plot, characters, and production details
- ğŸµ **Music & Poetry**: Detailed analysis of lyrics, musical themes, and poetic meaning
- ğŸ“° **News & Analysis**: Context-aware analysis of breaking news, interviews, and documentaries
- ğŸ”¬ **Technical Tutorials**: Step-by-step explanations for programming, design, and technical how-tos

---

## Key Innovation: Planet-Scale Vector Caching

### The Problem It Solves

Without vector caching, if a viral YouTube video gets 100 million views, the traditional approach would:
- Transcribe the video 100 million times
- Compute embeddings 100 million times
- Cost millions of dollars in API calls
- Create massive latency for each user

### The Solution: One-Time Processing, Infinite Distribution

YouTube Assistant implements a revolutionary architecture:

```
First User Queries Video
    â†“
[Transcript Fetched & Processed]
    â†“
[Embeddings Computed & Stored in Pinecone]
    â†“
[Cached with Video ID as Global Namespace]
    â†“
User 2, 3, 4... Million all use SAME cached data instantly
```

**Key Benefits:**

| Benefit | Impact |
|---------|--------|
| **One-Time Processing** | Transcription & embedding computation happens once per video |
| **Infinite Scalability** | 1 user or 1 billion users = same computational cost |
| **Massive Cost Savings** | 99.9% reduction in API costs for popular videos |
| **Sub-100ms Response** | All users after the first get instant, cached answers |
| **Global Knowledge Graph** | Every interaction enriches the shared knowledge pool |
| **Collaborative Annotations** | Future features can add community knowledge layers |

---

## Features

### Core Capabilities
- âœ… **Universal Video Support**: Works with any YouTube videoâ€”watch pages, shorts, embedded videos
- âœ… **Multi-Mode Transcript Retrieval**: Official captions when available; intelligent ASR fallback
- âœ… **Semantic Search**: Vector similarity search finds most relevant video segments
- âœ… **Intelligent Query Classification**: Broad queries (summarize) vs. specific queries (find X) handled optimally
- âœ… **Session-Based Chat Memory**: Multi-turn conversations with full context awareness
- âœ… **Voice Input & Output**: Web Speech API for hands-free queries, Text-to-Speech for answers

### Technical Features
- âœ… **Efficient Audio Chunking**: Large videos split into 5-minute segments for ASR processing
- âœ… **Overlapping Text Chunks**: Contextually-aware text splitting with 50-token overlap
- âœ… **Multi-Language Support**: Transcript detection for English, Hindi, Spanish, French, German, Russian, Portuguese, Japanese, Korean, Mandarin, Arabic
- âœ… **CORS-Enabled**: Seamless extension-to-backend communication
- âœ… **Namespace Isolation**: Each video has isolated vector space for security and efficiency
- âœ… **Conversation Persistence**: In-memory chat history per session

### User Experience Features
- âœ… **Floating UI Widget**: Non-intrusive, draggable, minimizable panel
- âœ… **Real-Time Character Counter**: Live feedback as users type
- âœ… **Loading Animations**: "Thinking" indicators for perceived performance
- âœ… **Responsive Design**: Dark theme optimized for YouTube interface
- âœ… **Error Handling**: Graceful degradation when APIs unavailable

---

## Technology Stack

### Backend

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Web Framework** | FastAPI (Python 3.x) | High-performance async API server |
| **LLM** | OpenAI GPT-4o-mini | Answer generation with reasoning |
| **Embeddings** | OpenAI text-embedding-ada-002 | Semantic vector representations |
| **Vector Database** | Pinecone | Distributed vector storage & retrieval |
| **Orchestration** | LangChain v0.1+ | Chain composition, memory management |
| **Transcript Fetching** | youtube-transcript-api | Official YouTube captions |
| **Audio Download** | yt-dlp | Download best audio from YouTube |
| **Speech Recognition** | faster-whisper (OpenAI Whisper) | Efficient ASR transcription |
| **Audio Processing** | pydub | Audio splitting, format conversion |
| **Text Splitting** | LangChain RecursiveCharacterTextSplitter | Intelligent text chunking |
| **Environment** | python-dotenv | Secure API key management |

### Frontend (Chrome Extension)

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Manifest** | Manifest v3 | Modern Chrome Extension API |
| **Content Script** | Vanilla JavaScript | DOM injection, UI rendering |
| **Background Service Worker** | Vanilla JavaScript | Message relay, API communication |
| **Styling** | Pure CSS | Dark theme, responsive layout |
| **Speech API** | Web Speech API | Voice recognition (SpeechRecognition) |
| **Text-to-Speech** | Web Speech API (SpeechSynthesis) | Answer vocalization |

### Infrastructure

| Service | Purpose | Pricing Model |
|---------|---------|---------------|
| **OpenAI API** | LLM + Embeddings | Pay-per-token |
| **Pinecone** | Vector Database | Serverless pay-per-request |
| **Render / Cloud Run** | Backend Hosting | Optional deployment |

---

## Architecture

### High-Level System Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        YouTube Website                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Chrome Extension (Content Script)            â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ UI Widget (HTML/CSS)                      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Video ID Extractor                        â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Voice Input Handler (Web Speech API)      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Message Sender to Background Script       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Answer Renderer + Voice Output            â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                        â†“ Message Bus                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚      Chrome Background Service Worker               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Listens for content script messages      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Fetches from backend API                 â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Relays response back                     â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“ HTTP POST (JSON)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Backend Server (FastAPI)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              POST /chat Endpoint                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚  Request: {video_id, session_id, question}  â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  1. Check retriever cache for video         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  2. If miss: fetch transcript               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  3. Split & embed chunks (â†’ Pinecone)       â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  4. Classify query (broad vs specific)      â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  5. Retrieve relevant context               â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  6. Generate response (GPT-4o-mini)         â”‚    â”‚   â”‚
â”‚  â”‚  â”‚  7. Return answer                           â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â”‚              CORS Middleware: *                       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ API Calls (Conditional)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      External API Services              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ OpenAI API                             â”‚
â”‚   - gpt-4o-mini (answer generation)      â”‚
â”‚   - text-embedding-ada-002 (vectors)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ YouTube Official                       â”‚
â”‚   - Transcript API (fetch captions)      â”‚
â”‚   - Audio download via yt-dlp            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Pinecone Vector Database               â”‚
â”‚   - Serverless vector index              â”‚
â”‚   - CoSine similarity search             â”‚
â”‚   - Namespace isolation per video        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
youtube_assistant/
â”œâ”€â”€ README.md                            # Main documentation
â”œâ”€â”€ README_DETAILED.md                   # This comprehensive guide
â”œâ”€â”€ .env.sample                          # Template for environment variables
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py                      # Package initialization
â”‚   â”œâ”€â”€ main.py                          # Core logic (transcripts, chunking, embeddings, prompt)
â”‚   â”‚   â”œâ”€â”€ get_transcript()             # Fetch captions OR transcode audio
â”‚   â”‚   â”œâ”€â”€ get_audio_from_youtube()     # Download audio via yt-dlp
â”‚   â”‚   â”œâ”€â”€ split_audio()                # Chunk large audio files (5-min chunks)
â”‚   â”‚   â”œâ”€â”€ merge_and_split_segments()   # Semantic text chunking
â”‚   â”‚   â”œâ”€â”€ chain_with_memory            # LangChain conversation chain
â”‚   â”‚   â””â”€â”€ get_session_history()        # Session memory management
â”‚   â”‚
â”‚   â”œâ”€â”€ index.py                         # FastAPI server & orchestration
â”‚   â”‚   â”œâ”€â”€ app = FastAPI()              # FastAPI application instance
â”‚   â”‚   â”œâ”€â”€ is_broad_query()             # Query classification regex patterns
â”‚   â”‚   â”œâ”€â”€ POST /chat                   # Main endpoint
â”‚   â”‚   â”œâ”€â”€ HEAD /uptime                 # Uptime ping (Render.com keep-alive)
â”‚   â”‚   â”œâ”€â”€ retriever_cache              # In-memory cache for vectorstores
â”‚   â”‚   â”œâ”€â”€ transcript_cache             # Full transcript storage per video
â”‚   â”‚   â””â”€â”€ Pinecone integration         # Index creation, embedding
â”‚   â”‚
â”‚   â”œâ”€â”€ requirements.txt                 # Python dependencies
â”‚   â””â”€â”€ __pycache__/                     # Compiled Python bytecode
â”‚
â””â”€â”€ chrome_extension/
    â”œâ”€â”€ manifest.json                    # Chrome extension config (v3)
    â”œâ”€â”€ content.js                       # Injected into YouTube pages
    â”œâ”€â”€ background.js                    # Service Worker (non-persistent)
    â””â”€â”€ icons/                           # Extension icons (48x48, 128x128)
```

---

## Deep Technical Details

### Backend Architecture

The backend is built with a **three-tier** architecture:

1. **API Layer** (`index.py`): FastAPI endpoints, request validation, CORS
2. **Processing Layer** (`main.py`): Transcript retrieval, chunking, LLM chains
3. **Storage & External Services**: Pinecone (vectors), OpenAI (LLM + embeddings), YouTube (captions/audio)

#### Key Design Patterns

| Pattern | Implementation | Benefit |
|---------|----------------|---------|
| **Caching** | In-memory Python dicts (`retriever_cache`, `transcript_cache`) | Avoids re-embedding same video |
| **Lazy Loading** | Retrieve transcript only on first `/chat` request for video | Reduces initial API costs |
| **Namespace Isolation** | Pinecone namespace = video_id | Security + efficient multi-tenancy |
| **Session Management** | Dictionary of `InMemoryChatMessageHistory` keyed by session_id | Per-user conversation context |
| **Query Classification** | Regex pattern matching on question | Route broad vs. specific queries |
| **Async I/O** | FastAPI async handlers + await on external APIs | Handle concurrent requests |

### Chrome Extension Architecture

The extension uses a **message-passing pattern** with strict separation:

```
User Types Question in Widget (content.js)
         â†“
    Sends Message
         â†“
Background Service Worker Receives (background.js)
         â†“
    Fetch to Backend
         â†“
    Receives Answer
         â†“
    Sends Back to Content Script
         â†“
Answer Rendered in Widget (content.js)
        â†“
Optional Voice Output (Web Speech API)
```

#### Why This Architecture?

- **Security**: Content script can't directly access external APIs
- **Isolation**: Background worker isolated from YouTube's JavaScript context
- **Reliability**: If content script crashes, background & API communication unaffected
- **Reusability**: Background script can handle multiple tabs/windows

### Data Flow Diagram

```
User Asks Question
    â†“
content.js extracts video_id & creates message
    â†“
Sends chrome.runtime.sendMessage()
    â†“
background.js receives message
    â†“
Fetches to FastAPI /chat endpoint
    â†“
POST Request with {video_id, session_id, question}
    â†“
Backend Main Processing:
  1. Check cache hit for video
  2. If miss: Fetch transcript (captions or ASR)
  3. If miss: Embed chunks & store in Pinecone
  4. Classify query (broad vs specific)
  5. Retrieve relevant context
  6. Generate answer with GPT-4o-mini
    â†“
Response returned as JSON {answer: "..."}
    â†“
background.js sends response back to content.js
    â†“
content.js renders answer in widget UI
    â†“
Optional: Synthesize voice output
```

---

## Component-by-Component Breakdown

### Transcript Acquisition Pipeline

**File**: `backend/main.py` â†’ `get_transcript(video_id, video_url)`

#### Step 1: Try Official Captions

```python
try:
    fetched_transcript = ytt_api.fetch(
        video_id,
        languages=["en", "hi", "es", "fr", "de", "ru", "pt", "ja", "ko", "zh-Hans", "ar"]
    )
```

**What happens**:
- Queries YouTube Transcript API for captions in preferred languages
- Returns list of caption objects with `text`, `start`, `duration` fields
- **Cost**: Free (uses public YouTube API)
- **Speed**: <1 second for most videos
- **Success rate**: ~80% of videos have captions

#### Step 2: Fallback to ASR (Audio Speech Recognition)

```python
except TranscriptsDisabled:
    print("No captions available, falling back to ASR...")
    audio_file = get_audio_from_youtube(video_url, output_path=f"file{video_id}.mp3")
```

**What happens**:
1. Downloads best available audio track from YouTube using `yt-dlp`
2. Stores as MP3 in local storage
3. **Cost**: Free (yt-dlp is open source)
4. **Speed**: 5-15 seconds (depends on video length and network)

#### Step 3: Audio Size Check & Splitting

```python
audio_size = os.path.getsize(audio_file) / (1024 * 1024)  # Size in MB

if audio_size < 25:
    segments, _ = model.transcribe(audio_file)
else:
    chunks = split_audio(audio_file, chunk_length_ms=300000)  # 5-min chunks
    for chunk in chunks:
        segments, _ = model.transcribe(chunk)
```

**Why split audio?**
- Whisper works best with audio <25 minutes to avoid memory issues
- Each 5-minute chunk is processed independently
- Results are merged with adjusted timestamps

**Configuration**:
- Chunk length: 5 minutes (300,000 ms)
- Model: `faster-whisper` with `tiny` model
- Compute type: `int8` (quantized for CPU efficiency)
- Device: CPU (GPU optional for speedup)

#### Output Format

Both paths return consistent format:

```python
[
    {
        "text": "Hello world, this is...",
        "start": 0.5,
        "duration": 2.3
    },
    {
        "text": "the next segment...",
        "start": 2.8,
        "duration": 1.5
    }
]
```

### Semantic Chunking & Embedding

**File**: `backend/main.py` â†’ `merge_and_split_segments()`

#### Stage 1: Merge Segments

Raw transcripts have many short segments. First merges overlapping segments:

```python
for seg in segments:
    if current and seg["start"] <= current["end"]:
        current["text"] += " " + seg["text"]
    else:
        merged.append(current)
        current = new_segment
```

#### Stage 2: Recursive Character Splitting

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=250,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", "à¥¤", "?", "!", " ", ""]
)
```

**How it works**:
- Splits on paragraph breaks first, then sentences, then words
- Creates overlapping chunks (50 chars = ~5-10 words)
- Preserves context across boundaries

#### Stage 3: Embedding Generation

```python
vectorstore = PineconeVectorStore.from_texts(
    texts=[chunk["text"] for chunk in chunks],
    embedding=OpenAIEmbeddings(model="text-embedding-ada-002"),
    namespace=video_id
)
```

**What happens**:
- Each chunk â†’ 1536-dimensional vector
- Vectors stored in Pinecone under video_id namespace
- Metadata (timestamp) attached for sourcing

### Vector Store Management

**File**: `backend/index.py` â†’ Pinecone integration

#### Caching Strategy

```python
retriever_cache = {}
transcript_cache = {}

if video_id not in retriever_cache:
    # Compute & store
    vectorstore = PineconeVectorStore.from_texts(...)
    retriever_cache[video_id] = vectorstore.as_retriever()
else:
    # Use cached retriever
    retriever = retriever_cache[video_id]
```

**Lifetime**: Duration of backend process (reset on deploy)

### Query Intelligence & Retrieval

**File**: `backend/index.py` â†’ Query classification + retrieval

#### Query Classification

```python
def is_broad_query(question: str) -> bool:
    broad_patterns = [
        r"explain (the )?video",
        r"summarize",
        r"what('?s| is) (this|the) video about",
        r"give (me )?(an )?overview",
        r"tell me about (this|the) video"
    ]
    return any(re.search(p, question.lower()) for p in broad_patterns)
```

#### Retrieval Strategy

**For Broad Queries**: Use full transcript
**For Specific Queries**: Vector similarity search (k=6 results)

---

## Setup & Installation

### Backend Setup

#### 1. Clone Repository

```bash
git clone https://github.com/gargdivyansh1/YouTube_Assistant.git
cd YouTube_Assistant
```

#### 2. Create Virtual Environment

```bash
python -m venv venv

# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

#### 3. Install Dependencies

```bash
cd backend
pip install -r requirements.txt
```

#### 4. Configure Environment

```bash
cp .env.sample .env
```

Edit `.env`:

```bash
OPENAI_API_KEY=sk-proj-xxxx...
PINECONE_API_KEY=pcak_xxxx...
```

#### 5. Start Backend

```bash
cd backend
python index.py
```

### Chrome Extension Setup

1. Open `chrome://extensions`
2. Enable "Developer Mode"
3. Click "Load Unpacked"
4. Select `chrome_extension/` folder
5. Navigate to YouTube video
6. Widget should appear (bottom-right)

---

## Configuration

### Backend (.env)

```bash
# Required
OPENAI_API_KEY=sk-proj-...
PINECONE_API_KEY=pcak_...

# Optional
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0
```

### Extension

Edit `chrome_extension/background.js`:

```javascript
const BACKEND_URL = "http://127.0.0.1:8000/chat";
```

---

## API Reference

### POST /chat

**Purpose**: Submit question about video

**Request**:
```json
{
  "session_id": "user_video_id",
  "question": "What is this about?",
  "video_id": "dQw4w9WgXcQ"
}
```

**Response**:
```json
{
  "answer": "This video is about..."
}
```

### HEAD /uptime

**Purpose**: Health check

**Response**: `{"status": "alive"}`

---

## Advanced Topics

### Session Management

Each `session_id` maintains separate chat history:

```python
store = {}
def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]
```

### Custom System Prompts

Edit `backend/main.py`:

```python
prompt = ChatPromptTemplate.from_template("""
You are a helpful AI assistant...
[CUSTOMIZE YOUR INSTRUCTIONS HERE]
""")
```

### Vector Search Optimization

```python
vectorstore.as_retriever(
    search_type='mmr',  # Maximum Marginal Relevance
    search_kwargs={'k': 6, 'lambda_mult': 0.5}
)
```

---

## Troubleshooting

### "Backend Connection Failed"

```bash
# Check backend is running
curl http://127.0.0.1:8000/uptime

# Restart
python backend/index.py
```

### "No Transcript Available"

```bash
# Update yt-dlp
pip install --upgrade yt-dlp

# Verify video is downloadable
yt-dlp https://www.youtube.com/watch?v=VIDEO_ID
```

### "API Key Invalid"

```bash
# Verify .env exists
cat ../.env

# Check format:
# OpenAI: sk-proj-XXXXX
# Pinecone: pcak_XXXXX
```

### "Timeout: Backend Taking Too Long"

First request for video: 30-60 seconds (expected)
Subsequent requests: <5 seconds (cached)

---

## Contributing

1. Fork repository
2. Create feature branch: `git checkout -b feature/your-feature`
3. Make changes (follow PEP 8)
4. Test on actual YouTube videos
5. Commit: `git commit -m "feat: description"`
6. Push and create Pull Request

---

## Acknowledgements

- [OpenAI](https://openai.com/) - GPT-4o-mini & embeddings
- [Pinecone](https://www.pinecone.io/) - Vector database
- [LangChain](https://github.com/langchain-ai/langchain) - LLM orchestration
- [FastAPI](https://fastapi.tiangolo.com/) - Web framework
- [Whisper ASR](https://github.com/openai/whisper) - Speech recognition
- [youtube-transcript-api](https://github.com/jderose9/youtube-transcript-api)
- [yt-dlp](https://github.com/yt-dlp/yt-dlp)
- [pydub](https://github.com/jiaaro/pydub)

---

## License

MIT License - see LICENSE file

---

**Version**: 1.0.0 | **Updated**: February 12, 2026

**Ready to revolutionize your YouTube experience? ğŸš€**
